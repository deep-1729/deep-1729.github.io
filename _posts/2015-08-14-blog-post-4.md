---
title: 'Graph Attention Networks'
date: 2015-08-14
permalink: /posts/2012/08/blog-post-4/
tags:
  - cool posts
  - category1
  - category2
---

The paper performs node classification of graph data by computing the hidden representations of each node by attending over its neighbors, following a self-attention strategy. Self-attention is an attention mechanism applied to compute a representation of a single sequence.  
GAT is a spatial method on graph-structured data. Performs well on both transductive and inductive learning. In transductive learning we already have both training and testing datasets when training the model. Inductive learning encounters only the training data when training the model and applies the learned model on a dataset never seen before. This article explains it better:


<!-- Headings are cool
======

You can have many headings
======

Aren't headings cool?
------ -->
